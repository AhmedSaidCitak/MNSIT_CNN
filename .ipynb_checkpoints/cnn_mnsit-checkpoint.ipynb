{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38e58d36-518b-4c6d-b8c2-f3ebff05ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f926f0df-475a-4883-ae1a-7cbf8b2fab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "valid_size = 0.1\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# choose the training and testing datasets\n",
    "train_data = datasets.MNIST(root = 'data', train = True, download = True, transform = transform)\n",
    "test_data = datasets.MNIST(root = 'data', train = False, download = True, transform = transform)\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_index, valid_index = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_index)\n",
    "valid_sampler = SubsetRandomSampler(valid_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e457139c-e499-40ad-ba0c-bff88bddd6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, sampler = train_sampler)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, sampler = valid_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23721d53-c7a1-48bd-bb86-4f7cd2fd244e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "training_data = enumerate(train_loader)\n",
    "batch_idx, (images, labels) = next(training_data)\n",
    "print(type(images)) # Checking the datatype \n",
    "print(images.shape) # the size of the image\n",
    "print(labels.shape) # the size of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94096dce-82fb-4d70-a4af-4cdbcbd069b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(32,64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(3*3*64, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(F.max_pool2d(self.conv3(x),2))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.view(-1,3*3*64 )\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    " \n",
    "model = CNN()\n",
    "print(model)\n",
    "\n",
    "it = iter(train_loader)\n",
    "X_batch, y_batch = next(it)\n",
    "print(model.forward(X_batch).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1d52607-adc3-43e9-9111-fccd99007835",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4543c135-a1c9-4316-9731-6b8a4128870f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.110213 \tValidation Loss: 0.000003 \tPatience Counter: 0\n",
      "Validation loss decreased (inf --> 0.000003).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.085875 \tValidation Loss: 0.000003 \tPatience Counter: 0\n",
      "Validation loss decreased (0.000003 --> 0.000003).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.076169 \tValidation Loss: 0.000003 \tPatience Counter: 0\n",
      "Epoch: 4 \tTraining Loss: 0.070347 \tValidation Loss: 0.000001 \tPatience Counter: 1\n",
      "Validation loss decreased (0.000003 --> 0.000001).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.068063 \tValidation Loss: 0.000381 \tPatience Counter: 0\n",
      "Epoch: 6 \tTraining Loss: 0.064357 \tValidation Loss: 0.000001 \tPatience Counter: 1\n",
      "Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.060497 \tValidation Loss: 0.000024 \tPatience Counter: 0\n",
      "Epoch: 8 \tTraining Loss: 0.058529 \tValidation Loss: 0.000113 \tPatience Counter: 1\n",
      "Epoch: 9 \tTraining Loss: 0.057320 \tValidation Loss: 0.000068 \tPatience Counter: 2\n",
      "Epoch: 10 \tTraining Loss: 0.057217 \tValidation Loss: 0.000006 \tPatience Counter: 3\n",
      "Epoch: 11 \tTraining Loss: 0.055341 \tValidation Loss: 0.000001 \tPatience Counter: 4\n",
      "Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.054283 \tValidation Loss: 0.000215 \tPatience Counter: 0\n",
      "Epoch: 13 \tTraining Loss: 0.051098 \tValidation Loss: 0.000001 \tPatience Counter: 1\n",
      "Epoch: 14 \tTraining Loss: 0.051413 \tValidation Loss: 0.000001 \tPatience Counter: 2\n",
      "Epoch: 15 \tTraining Loss: 0.051246 \tValidation Loss: 0.000057 \tPatience Counter: 3\n",
      "Epoch: 16 \tTraining Loss: 0.047666 \tValidation Loss: 0.000025 \tPatience Counter: 4\n",
      "Epoch: 17 \tTraining Loss: 0.046285 \tValidation Loss: 0.000021 \tPatience Counter: 5\n",
      "Epoch: 18 \tTraining Loss: 0.051897 \tValidation Loss: 0.000133 \tPatience Counter: 6\n",
      "Epoch: 19 \tTraining Loss: 0.047506 \tValidation Loss: 0.000000 \tPatience Counter: 7\n",
      "Validation loss decreased (0.000001 --> 0.000000).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.051707 \tValidation Loss: 0.000055 \tPatience Counter: 0\n",
      "Epoch: 21 \tTraining Loss: 0.049626 \tValidation Loss: 0.000003 \tPatience Counter: 1\n",
      "Epoch: 22 \tTraining Loss: 0.050282 \tValidation Loss: 0.000000 \tPatience Counter: 2\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.045976 \tValidation Loss: 0.000015 \tPatience Counter: 0\n",
      "Epoch: 24 \tTraining Loss: 0.049542 \tValidation Loss: 0.000227 \tPatience Counter: 1\n",
      "Epoch: 25 \tTraining Loss: 0.047575 \tValidation Loss: 0.000003 \tPatience Counter: 2\n",
      "Epoch: 26 \tTraining Loss: 0.046592 \tValidation Loss: 0.000008 \tPatience Counter: 3\n",
      "Epoch: 27 \tTraining Loss: 0.044955 \tValidation Loss: 0.000052 \tPatience Counter: 4\n",
      "Epoch: 28 \tTraining Loss: 0.046703 \tValidation Loss: 0.000002 \tPatience Counter: 5\n",
      "Epoch: 29 \tTraining Loss: 0.048388 \tValidation Loss: 0.000217 \tPatience Counter: 6\n",
      "Epoch: 30 \tTraining Loss: 0.046144 \tValidation Loss: 0.000007 \tPatience Counter: 7\n",
      "Epoch: 31 \tTraining Loss: 0.045363 \tValidation Loss: 0.000461 \tPatience Counter: 8\n",
      "Epoch: 32 \tTraining Loss: 0.045885 \tValidation Loss: 0.000015 \tPatience Counter: 9\n",
      "Epoch: 33 \tTraining Loss: 0.046275 \tValidation Loss: 0.000000 \tPatience Counter: 10\n",
      "Epoch: 34 \tTraining Loss: 0.044163 \tValidation Loss: 0.000062 \tPatience Counter: 11\n",
      "Epoch: 35 \tTraining Loss: 0.044851 \tValidation Loss: 0.000000 \tPatience Counter: 12\n",
      "Epoch: 36 \tTraining Loss: 0.044778 \tValidation Loss: 0.000000 \tPatience Counter: 13\n",
      "Epoch: 37 \tTraining Loss: 0.048335 \tValidation Loss: 0.000001 \tPatience Counter: 14\n",
      "Epoch: 38 \tTraining Loss: 0.043229 \tValidation Loss: 0.000140 \tPatience Counter: 15\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 1000\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor losses\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "         \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train() # prep model for training\n",
    "    for data,label in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output,label)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "                \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()  # prep model for evaluation\n",
    "    for data,label in valid_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output,label)\n",
    "        # update running validation loss \n",
    "        valid_loss = loss.item() * data.size(0)\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss / len(train_loader.sampler)\n",
    "    valid_loss = valid_loss / len(valid_loader.sampler)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tPatience Counter: {}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss,\n",
    "        patience_counter\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss < valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_cnn.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "        patience_counter = 0\n",
    "    elif patience_counter < patience:\n",
    "        patience_counter += 1\n",
    "    else:\n",
    "        break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1009ac5e-e626-4f59-aa04-2974377834d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.020378\n",
      "\n",
      "Test Accuracy of     0: 99% (976/980)\n",
      "Test Accuracy of     1: 99% (1132/1135)\n",
      "Test Accuracy of     2: 99% (1026/1032)\n",
      "Test Accuracy of     3: 99% (1008/1010)\n",
      "Test Accuracy of     4: 99% (978/982)\n",
      "Test Accuracy of     5: 98% (883/892)\n",
      "Test Accuracy of     6: 98% (947/958)\n",
      "Test Accuracy of     7: 99% (1021/1028)\n",
      "Test Accuracy of     8: 99% (971/974)\n",
      "Test Accuracy of     9: 99% (1000/1009)\n",
      "\n",
      "Test Accuracy (Overall): 99% (9942/10000)\n"
     ]
    }
   ],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "model.eval() # prep model for evaluation\n",
    "for data, target in test_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(len(target)):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89299e5a-8ed5-4aed-b218-91cb805b347f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b6a7d6d-3a89-4dd6-be56-4618130c698f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e525880-9b74-4207-a337-4a803fb220c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "type torch.cuda.FloatTensor not available. Torch not compiled with CUDA enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-e5f0863857e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: type torch.cuda.FloatTensor not available. Torch not compiled with CUDA enabled."
     ]
    }
   ],
   "source": [
    "a=torch.cuda.FloatTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aae4c4-a1a4-4668-bd74-294311e65b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
